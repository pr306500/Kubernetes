#. Feed the config file to the kubectl tool, below is the cmd to exec it:
   1. kubectl                  apply           -f          <filename> {path to the file with the config}
      {                        { 
                            change the current 
        CLI we use to       config of our cluster
    change our Kubernetes      }
       cluser 
      }
*. The cmd in order to fetch the status of the running pods: kubectl get pods
   NAME     READY	STATUS 		RESTARTS 	AGE
client-pod   1/1     Running 	   0         1h

 In case of "READY" 1/1, numerator 1 represents the number of pods that are running.
                         denominator 1 represents the number of copies that we want to have.
  
  If we want to scale our application we might want to have multiple copies of the exact same pod running.

*. "kubectl get services" cmd fetches all the status of the running services (type of object)


/********************************** Rest Info is written inside the NoteBook*****************************************/

*. cmd: minikube start, it helps us in creating the VM(Node).
*. When we ran the cmd <kubectl apply -f "client-pod.yaml">, the file gets passed to master. The kube-apiserver program is
   100% responsible for monitoring the current status of all diff nodes inside of the cluster & making sure that they are doing the correct
   thing. If we crash the container manually, it get restarted for us.
   In the deployment file ("client-pod.yaml") we could configure it in such a way that we could have 4 copies of "multi-worker" container.
   VM created by minikube has be-default docker up and running inside it. The Docker is going to reach out to Docker Hub n gonna find the 
   multi-worker image over there and download it over local cache inside the Node. Technically the containers will run inside the pod interface.
*. Anything that happens inside these Nodes, the master gets the notification so when we the cmd "docker kill <container_id>" one of the container
   inside the Node got killed as a result master gets the notification. Master re-check the config requirements like we need 4 copies but currently 
   only 3 are up and running hence master asks the nodes to run the additional copy of the "multi-worker" running.
   It's up to the master to reach out to some node and tell it to do some amount of work to fulfill the master's list of responsibilities, we cannot
   directly communicate to the nodes it culd only happen via master thrigh kubectl cli.
*. video(3:02), Node in Kubernetes world is a computer / VM that is going to run some number of objects that we created
   inside our cluster

   *****************************************Wednesday(11/27/2019)***************************************
   # Why use services??
    <kubectl get pods -o wide> fetches extra data in case of "deployment" object will get IP and node extra in the output.
    Evey pod that we create gets its own IP address assigned to it. This IP address is internal to our VM, so we cannot
    visit this IP easily.

    Port mapping Flow Diagram is Imp (192:3.37).

    The Service Object is going to watch for every pod that matches its selector, in our case service has a selector of component: web.
    So the service is going to look for the every pod with that selector and then automatically route traffic directly to it.

    # Scaling and Changing Deploments
  
  We modified the file config where to set th replicas value from 1 to 5 and re-deployed it. When we ran the cmd: kubectl get pods,
  we will see the 5 separate pods each of which is running a unique copy of that very specific multi-client.
    
    # Updating Deployment Images:

 **. How we can update a deployment when a new version of image becomes available ? (Tricky Part)
 Approach 1: 
   Manually delete pods to get the deployment(master) to recreate them with the latest version.

 Approach 2:
   Tag build images with a real version number & specify that version in the config file.
   So basically we are saying to first update the image with the version number via docker build cmd & then
   change the config file to  the version recently updated.  

 Approach 3: [Reasonable Solution]
   Use an imperative cmd to update the image version the deployment should use.  
 
  # Imperatively Updating a Deployment's image

  Update Image Version: 
  Tag the image with version number, push it to docker hub. >> Run a kubectl cmd forcing the deployment to
  use the new version image.

  => kubectl            set           image              <object_type> / <object_name>     <container_name>  = <new image to use>
  "cli we use to     " we want      "we want to change
   change our         to change      the image property"    "deployment"
   Kubernetes         a property "
   cluster"

   cmd: kubectl set image deployment/client-deployment client=stephengrider/multi-client:v5 

   